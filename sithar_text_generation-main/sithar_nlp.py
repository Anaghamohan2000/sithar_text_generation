# -*- coding: utf-8 -*-
"""sithar_nlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15Q-M5iDy6dS1GULIxCJ953SAG2lCRAQw
"""

from google.colab import drive
drive.mount('/content/drive/')

!pip install python-docx
!nvidia-smi

from tensorflow.keras.preprocessing.text import Tokenizer
import docx
import re
from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Dropout, Bidirectional
from tensorflow.keras.models import Model , load_model , model_from_json
from tensorflow.keras.optimizers import Adam
import random
import numpy as np
import tensorflow as tf
import os



class data_processing:
  
  def __init__(self,max_vocab , sequence_length):
    self.sequence_length = sequence_length
    self.max_vocab = max_vocab
    self.tokenizer = Tokenizer(num_words = self.max_vocab)
    self.text = self.getText("/content/drive/My Drive/sithara songs.docx")

    self.lower_data = self.text.lower()           
    self.split_data = self.lower_data.splitlines()      
    self.final = ''     

    for line in self.split_data:
        line = self.clean_text(line)
        self.final += '\n' + line

    self.final_data = self.final.split('\n')       
    self.tokenizer.fit_on_texts(self.final_data)
    self.word2idx = self.tokenizer.word_index
    self.idx2word = dict(map(reversed, self.word2idx.items()))
    self.words = self.word2idx.keys()
    self.words_indexes = [self.word2idx[w] for w in self.words]
    self.len_word = len(self.words_indexes)

  def data_xy(self,index):    
      x = self.words_indexes[index : index + self.sequence_length]
      y = self.words_indexes[index + 1 : index + self.sequence_length+1]
      return {"X":x , "Y":y}
   
  def getText(self,filename):
      doc = docx.Document(filename)
      fullText = []
      for para in doc.paragraphs:
          fullText.append(para.text)
      return '\n'.join(fullText)

  def clean_text(self,text):
      text = re.sub(r',', '', text)
      text = re.sub(r'\'', '',  text)
      text = re.sub(r'\"', '', text)
      text = re.sub(r'\(', '', text)
      text = re.sub(r'\)', '', text)
      text = re.sub(r'\n', '', text)
      text = re.sub(r'“', '', text)
      text = re.sub(r'”', '', text)
      text = re.sub(r'’', '', text)
      text = re.sub(r'\.', '', text)
      text = re.sub(r';', '', text)
      text = re.sub(r':', '', text)
      text = re.sub(r'\-', '', text)
      return text

  def data_generator(self,batch_size = 10):
    while True:
      x_batch = np.empty((batch_size, self.sequence_length), dtype='int64') 
      y_batch = np.empty((batch_size, self.sequence_length), dtype='int64') 
      randomlist = random.sample(range(self.len_word-self.sequence_length-1), batch_size)
      for i in randomlist:
        data = self.data_xy(i)
        x_batch[:] = data["X"]
        y_batch[:] = data["Y"]
      yield (x_batch , y_batch)

class MyModel(tf.keras.Model):
  
  def __init__(self, vocab_size, embedding_dim):
    super().__init__(self)
    self.Embedding = Embedding(vocab_size, embedding_dim)
    self.Dropout = Dropout(0.2)
    self.Bidirectional = Bidirectional(layer=LSTM(340, return_sequences=True))
    self.LSTM = LSTM(100 , return_sequences=True)
    self.Dense_1 = Dense(1024, activation='relu')
    self.Dense_2 = Dense(vocab_size, activation='softmax')
    self.loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)
    
  def forward(self,return_state=False):
    input = Input(shape=(20,))
    x = self.Embedding(input) 
    x = self.Bidirectional(x)
    x = self.Dropout(x)
    x = self.LSTM(x)
    x = self.Dense_1(x)
    x = self.Dropout(x)
    x = self.Dense_2(x)
    x = tf.keras.Model(input, x)
    x.compile(optimizer='adam', loss=self.loss)
    return x

class prediction:
  def __init__(self, model,tokenizer, idx2words ,temperature=1.0):
    
    self.temperature = temperature
    self.model = model
    self.tokenizer = tokenizer
    self.idx2word  = idx2word

  def pred_next_word(self, inputs):

    input_ids = self.tokenizer.word_index[inputs]
    input_ids = np.array([[input_ids]], dtype=np.float32)
    predicted_logits = self.model.predict(input_ids)
    predicted_logits = predicted_logits[:, -1, :]
    predicted_logits = predicted_logits/self.temperature

    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)
    predicted_ids = tf.squeeze(predicted_ids, axis=-1)
    predicted_chars = self.idx2word[int(np.squeeze(np.squeeze(predicted_ids)))]
    
    return predicted_chars

def train_model(max_vocab , sequence_length):

  df = data_processing(max_vocab=2101 , sequence_length = 20)
  sequence_length = 20
  EPOCHS = 100
  vocab_size = df.len_word
  class_model = MyModel(vocab_size,embedding_dim=256)
  model = class_model.forward()

  checkpoint_dir = '/content/drive/My Drive/training_checkpoints'
  checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")
  checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)

  history = model.fit_generator(df.data_generator(),epochs=EPOCHS,steps_per_epoch=50 ,callbacks=[checkpoint_callback])

  model_json = model.to_json()
  with open("/content/drive/My Drive/model/my_model.json", "w") as  json_file:
    json_file.write(model_json)
  model.save_weights("/content/drive/My Drive/model/my_model.h5")
  print("Saved model to disk")

  return model , df.tokenizer,df.idx2word

if __name__=="__main__":

  model_prediction ,tokenizer ,idx2word = train_model(max_vocab =2101 ,sequence_length = 256)

  json_file = open('/content/drive/My Drive/model/my_model.json', 'r')
  loaded_model_json = json_file.read()
  json_file.close()

  loaded_model = model_from_json(loaded_model_json)
  loaded_model.load_weights("/content/drive/My Drive/model/my_model.h5") 
  print("Loaded model from disk")
  
  loaded_model.summary()

  pred = prediction(loaded_model , tokenizer , idx2word)

  next_char = input("enter the keyword")
  result = [next_char]

  for n in range(20):
    next_char = pred.pred_next_word(next_char)
    result.append(" ")
    result.append(next_char)

  result = tf.strings.join(result)
  print(result.numpy().decode('utf-8'), '\n\n' + '_'*80)

